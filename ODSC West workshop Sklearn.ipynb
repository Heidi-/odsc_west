{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bbc-text.csv\")\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list = stopword_list + [\"said\", \"also\", \"would\", \"first\", \"last\", \"one\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tech', 'politics', 'sport', 'business', 'entertainment'}\n"
     ]
    }
   ],
   "source": [
    "print(set(df['category'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words\n",
    "def get_stats(words, num_words=200):\n",
    "    #words = [word for word in words if word not in stopword_list]\n",
    "    #words = [word for word in words if re.search(\"[A-Za-z]\", word)]\n",
    "    freq_dist = FreqDist(words)\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    print(df['text'])\n",
    "    df['word_list'] = df['text'].apply(lambda x: nltk.tokenize.word_tokenize(x))\n",
    "    df['word_list'] = df['word_list'].apply(lambda x: [word for word in x if word not in stopword_list])\n",
    "    df['word_list'] = df['word_list'].apply(lambda x: [word for word in x if re.search(\"[A-Za-z]\", word)])\n",
    "    df['lemmas'] = df['word_list'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       tv future in the hands of viewers with home th...\n",
      "1       worldcom boss  left books alone  former worldc...\n",
      "2       tigers wary of farrell  gamble  leicester say ...\n",
      "3       yeading face newcastle in fa cup premiership s...\n",
      "4       ocean s twelve raids box office ocean s twelve...\n",
      "                              ...                        \n",
      "2220    cars pull down us retail figures us retail sal...\n",
      "2221    kilroy unveils immigration policy ex-chatshow ...\n",
      "2222    rem announce new glasgow concert us band rem h...\n",
      "2223    how political squabbles snowball it s become c...\n",
      "2224    souness delight at euro progress boss graeme s...\n",
      "Name: text, Length: 2225, dtype: object\n",
      "           category                                               text  \\\n",
      "0              tech  tv future in the hands of viewers with home th...   \n",
      "1          business  worldcom boss  left books alone  former worldc...   \n",
      "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
      "3             sport  yeading face newcastle in fa cup premiership s...   \n",
      "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
      "...             ...                                                ...   \n",
      "2220       business  cars pull down us retail figures us retail sal...   \n",
      "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
      "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
      "2223       politics  how political squabbles snowball it s become c...   \n",
      "2224          sport  souness delight at euro progress boss graeme s...   \n",
      "\n",
      "                                              word_list  \\\n",
      "0     [tv, future, hands, viewers, home, theatre, sy...   \n",
      "1     [worldcom, boss, left, books, alone, former, w...   \n",
      "2     [tigers, wary, farrell, gamble, leicester, say...   \n",
      "3     [yeading, face, newcastle, fa, cup, premiershi...   \n",
      "4     [ocean, twelve, raids, box, office, ocean, twe...   \n",
      "...                                                 ...   \n",
      "2220  [cars, pull, us, retail, figures, us, retail, ...   \n",
      "2221  [kilroy, unveils, immigration, policy, ex-chat...   \n",
      "2222  [rem, announce, new, glasgow, concert, us, ban...   \n",
      "2223  [political, squabbles, snowball, become, commo...   \n",
      "2224  [souness, delight, euro, progress, boss, graem...   \n",
      "\n",
      "                                                 lemmas  \n",
      "0     [tv, future, hand, viewer, home, theatre, syst...  \n",
      "1     [worldcom, bos, left, book, alone, former, wor...  \n",
      "2     [tiger, wary, farrell, gamble, leicester, say,...  \n",
      "3     [yeading, face, newcastle, fa, cup, premiershi...  \n",
      "4     [ocean, twelve, raid, box, office, ocean, twel...  \n",
      "...                                                 ...  \n",
      "2220  [car, pull, u, retail, figure, u, retail, sale...  \n",
      "2221  [kilroy, unveils, immigration, policy, ex-chat...  \n",
      "2222  [rem, announce, new, glasgow, concert, u, band...  \n",
      "2223  [political, squabble, snowball, become, common...  \n",
      "2224  [souness, delight, euro, progress, bos, graeme...  \n",
      "\n",
      "[2225 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = preprocess(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = [word for article in df['word_list'].values for word in article]\n",
    "fd = get_stats(flattened)\n",
    "#print(fd.most_common(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams\n",
    "def get_ngrams(df, input_col_name, output_col_name, ngram_func):\n",
    "    df[output_col_name] = df[input_col_name].apply(lambda x: list(ngram_func(x)))\n",
    "    df[output_col_name] = df[output_col_name].apply(lambda x: [bigram for bigram in x \n",
    "                                                   if bigram[0] not in stopword_list\n",
    "                                                  and bigram[1] not in stopword_list])\n",
    "    df[output_col_name] = df[output_col_name].apply(lambda x: [bigram for bigram in x \n",
    "                                                   if re.search(\"[A-Za-z]\", bigram[0])\n",
    "                                                  and re.search(\"[A-Za-z]\", bigram[1])])\n",
    "    df[output_col_name] = df[output_col_name].apply(lambda x: [\" \".join(bigram) for bigram in x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_ngrams(df, \"word_list\", \"bigrams\", nltk.bigrams)\n",
    "flattened_bigrams = [ngram for article in df['bigrams'].values for ngram in article]\n",
    "fd = get_stats(flattened_bigrams)\n",
    "#print(fd.most_common(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_ngrams(df, \"word_list\", \"trigrams\", nltk.trigrams)\n",
    "flattened_trigrams = [ngram for article in df['trigrams'].values for ngram in article]\n",
    "fd = get_stats(flattened_trigrams)\n",
    "#print(fd.most_common(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['lemmas'])\n",
    "flattened_lemmas = [lemma for article in df['lemmas'].values for lemma in article]\n",
    "fd = get_stats(flattened_lemmas)\n",
    "#print(fd.most_common(num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(sentence):\n",
    "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in stopword_list and t not in string.punctuation and re.search('[a-zA-Z]', t)]\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in stopword_list and t not in string.punctuation and re.search('[a-zA-Z]', t)]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(text_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_df=0.9, min_df=0.02, \n",
    "                                       ngram_range=(1,2), stop_words=stopword_list, tokenizer=tokenize_and_stem)\n",
    "    data = tfidf_vectorizer.fit_transform(text_list)\n",
    "    return (data, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "(vect_data, vectorizer) = create_vectorizer(df['text'].values)\n",
    "#print(list(vectorizer.vocabulary_.keys())[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv', 'futur', 'hand', 'viewer', 'home', 'theatr', 'system', 'digit', 'video', 'record', 'move', 'live', 'room', 'way', 'peopl', 'watch', 'differ', 'five', 'year', 'time', 'accord', 'expert', 'panel', 'gather', 'annual', 'consum', 'electron', 'show', 'discuss', 'new', 'technolog', 'impact', 'favourit', 'us', 'lead', 'trend', 'programm', 'content', 'deliv', 'via', 'network', 'cabl', 'telecom', 'compani', 'broadband', 'servic', 'provid', 'front', 'portabl', 'devic', 'person', 'box', 'like', 'uk', 'allow', 'store', 'play', 'forward', 'want', 'essenti', 'much', 'set', 'big', 'busi', 'japan', 'take', 'europ', 'lack', 'program', 'channel', 'schedul', 'put', 'togeth', 'entertain', 'worri', 'mean', 'term', 'revenu', 'well', 'brand', 'although', 'moment', 'concern', 'rais', 'particular', 'grow', 'happen', 'today', 'see', 'nine', 'month', 'bbc', 'broadcast', 'told', 'news', 'websit', 'issu', 'lost', 'yet', 'press']\n"
     ]
    }
   ],
   "source": [
    "print(list(vectorizer.vocabulary_.keys())[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_fit_lda_sklearn(data, num_topics):\n",
    "    lda = LDA(n_components=num_topics)\n",
    "    lda.fit(data)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = create_and_fit_lda_sklearn(vect_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words_for_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names()\n",
    "    word_dict = {}\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        this_topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        word_dict[topic_index] = this_topic_words\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_words(word_dict):\n",
    "    for key in word_dict.keys():\n",
    "        print(f\"Topic {key}\")\n",
    "        print(\"\\t\", word_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "\t ['game', 'play', 'win', 'player', 'england', 'club', 'match', 'injuri', 'champion', 'cup']\n",
      "Topic 1\n",
      "\t ['trial', 'suspect', 'pcs', 'injur', 'arrest', 'wale', 'b', 'fox', 'format', 'tie']\n",
      "Topic 2\n",
      "\t ['mr', 'labour', 'elect', 'parti', 'blair', 'govern', 'tori', 'minist', 'brown', 'lord']\n",
      "Topic 3\n",
      "\t ['film', 'award', 'star', 'best', 'music', 'show', 'band', 'actor', 'album', 'nomin']\n",
      "Topic 4\n",
      "\t ['compani', 'us', 'firm', 'year', 'use', 'market', 'mr', 'peopl', 'sale', 'share']\n"
     ]
    }
   ],
   "source": [
    "topic_words = get_most_common_words_for_topics(lda, vectorizer, 10)\n",
    "print_topic_words(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with a new example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_example = \"\"\"Manchester United players slumped to the turf \n",
    "at full-time in Germany on Tuesday in acknowledgement of what their \n",
    "latest pedestrian first-half display had cost them. The 3-2 loss at \n",
    "RB Leipzig means United will not be one of the 16 teams in the draw \n",
    "for the knockout stages of the Champions League. And this is not the \n",
    "only price for failure. The damage will be felt in the accounts, in \n",
    "the dealings they have with current and potentially future players \n",
    "and in the faith the fans have placed in manager Ole Gunnar Solskjaer. \n",
    "With Paul Pogba's agent angling for a move for his client and ex-United \n",
    "defender Phil Neville speaking of a \"witchhunt\" against his former team-mate \n",
    "Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(lda, vect, example):\n",
    "    vectorized = vect.transform([example])\n",
    "    topic = lda.transform(vectorized)\n",
    "    print(topic)\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78915453 0.02937867 0.02984918 0.02977619 0.12184142]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.78915453, 0.02937867, 0.02984918, 0.02977619, 0.12184142]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_example(lda, vectorizer, new_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lda, lda_path, vect, vect_path):\n",
    "    pickle.dump(lda, open(lda_path, 'wb'))\n",
    "    pickle.dump(vect, open(vect_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
